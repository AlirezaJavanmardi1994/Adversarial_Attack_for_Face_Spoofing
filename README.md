# Adversarial_Attack_for_Face_Spoofing
Three types of adversarial attacks including FGSM, PGD and DeepFool has been exploited to attack a face spoofing detection model
Despite resulted achievements in deep learning fields, deep neural networks have always been facing several threats. We are noticing that adding slight imperceptible adversarial perturbations can fool a highly accurate trained model. Biometric field in which accuracy and reliability has prominent importance also encounter dangerous attacks. With advent and spread of face recognition mechanisms and easy access to individual's picture through social networks or captured by cameras and mobile phones, attackers gain more ability to use this fake data which increase their chance of illegal access to critical social and government security accounts. Researchers in this field introduce face spoofing detection networks to remove or highly reduce the risk of spoofing attack leading to unauthorized access. However, adversarial attacks pose potential threats to this networks because deep neural networks are vulnerable to even small adversarial perturbations which in biometric systems leads to wrong prediction and misclassification of fake images as real ones and allow access to confidential information and cause catastrophic events. In this paper, by considering three famous databases in presentation attack detection, model performance in detecting fake faces has been evaluated against three adversarial attacks, In other words, a spoofed image which was correctly classified as fake, can be easily misclassified as real image by adding slight perturbations.
![alt text](https://www.uplooder.net/files/445eef5e0e88cd96c242314e530adee9/shape-1.tif.html)

